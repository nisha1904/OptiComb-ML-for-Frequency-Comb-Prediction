{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d58648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd57641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as rf_dc_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Lists to store dataset entries\n",
    "data = []\n",
    "\n",
    "# User input for fixed RF value\n",
    "fixed_type = input(\"Enter 'RF1' to fix RF1 or 'RF2' to fix RF2: \").strip().upper()\n",
    "rf_freq=float(input(\"enter the frequency value\"))\n",
    "if fixed_type == \"RF2\":\n",
    "    fixed_RF2 = float(input(\"Enter the fixed RF2 value: \"))\n",
    "    rf1_values = np.arange(1, 4.25, 0.25)\n",
    "    dc_values = np.arange(3, -0.25, -0.25)\n",
    "    for rf1, dc in zip(rf1_values, dc_values):\n",
    "        frequencies = [193.1E+12 + n * rf_freq for n in range(-50,51,1)]\n",
    "        data.append([rf1, fixed_RF2, dc] + frequencies)\n",
    "\n",
    "elif fixed_type == \"RF1\":\n",
    "    fixed_RF1 = float(input(\"Enter the fixed RF1 value: \"))\n",
    "    rf2_values = np.arange(0, 5.125, 0.125)\n",
    "    dc_values = np.arange(4, -0.125, -0.125)\n",
    "    for rf2, dc in zip(rf2_values, dc_values):\n",
    "        frequencies = [193.1E+12 + n * rf_freq for n in range(-50,51,1)]\n",
    "        data.append([fixed_RF1, rf2, dc] + frequencies)\n",
    "else:\n",
    "    print(\"Invalid input. Please enter 'RF1' or 'RF2'.\")\n",
    "    exit()\n",
    "\n",
    "# Create DataFrame with frequency column names\n",
    "df_columns = [\"RF1\", \"RF2\", \"DC Value\"] + [f\"Freq_{n}\" for n in range(-50,51)]\n",
    "df = pd.DataFrame(data, columns=df_columns)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"rf_dc_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Dataset saved as rf_dc_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394cd56a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Frequency'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries(power_values)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Apply function to map power values\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m power_columns \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_power_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([dataset, power_columns], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Save the updated dataset\u001b[39;00m\n",
      "File \u001b[1;32md:\\shadow_knight\\Opticomb\\venv\\lib\\site-packages\\pandas\\core\\frame.py:10381\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10367\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10369\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10371\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10379\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10380\u001b[0m )\n\u001b[1;32m> 10381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\shadow_knight\\Opticomb\\venv\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\shadow_knight\\Opticomb\\venv\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32md:\\shadow_knight\\Opticomb\\venv\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mmap_power_values\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     22\u001b[0m freq_value \u001b[38;5;241m=\u001b[39m row[freq_col]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Find the power value for the matching frequency\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m power_match \u001b[38;5;241m=\u001b[39m rf2_data\u001b[38;5;241m.\u001b[39mloc[\u001b[43mrf2_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFrequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m freq_value, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPower\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Assign the power value or NaN if not found\u001b[39;00m\n\u001b[0;32m     28\u001b[0m power_values[power_col] \u001b[38;5;241m=\u001b[39m power_match\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m power_match\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[1;32md:\\shadow_knight\\Opticomb\\venv\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\shadow_knight\\Opticomb\\venv\\lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Frequency'"
     ]
    }
   ],
   "source": [
    "# Load the main dataset\n",
    "dataset = pd.read_csv(\"rf_dc_dataset.csv\")\n",
    "\n",
    "# Load the Excel file with multiple RF2 sheets\n",
    "excel_file = './data/pm.xlsx' # Change to your actual file name\n",
    "excel_data = pd.ExcelFile(excel_file)  # Load the excel file\n",
    "sheets = excel_data.sheet_names  # Get all sheet names\n",
    "\n",
    "# Iterate through each row in the dataset and map power values\n",
    "def map_power_values(row):\n",
    "    rf2_value = str(row[\"RF2\"])  # Extract RF2 value and convert to string for sheet matching\n",
    "    power_values = {}\n",
    "\n",
    "    if rf2_value in sheets:  # Check if RF2 sheet exists\n",
    "        # Get RF2 data from the corresponding sheet using pandas.read_excel\n",
    "        rf2_data = pd.read_excel(excel_file, sheet_name=rf2_value)\n",
    "\n",
    "        for n in range(-50, 51):  # Iterate over all frequency columns\n",
    "            freq_col = f\"Freq_{n}\"\n",
    "            power_col = f\"Power_{n}\"\n",
    "\n",
    "            freq_value = row[freq_col]\n",
    "\n",
    "            # Find the power value for the matching frequency\n",
    "            power_match = rf2_data.loc[rf2_data['Frequency'] == freq_value, 'Power']\n",
    "\n",
    "            # Assign the power value or NaN if not found\n",
    "            power_values[power_col] = power_match.iloc[0] if not power_match.empty else np.nan\n",
    "    else:\n",
    "        for n in range(-50, 51):\n",
    "            power_values[f\"Power_{n}\"] = np.nan  # Assign NaN if RF2 sheet not found\n",
    "\n",
    "    return pd.Series(power_values)\n",
    "\n",
    "# Apply function to map power values\n",
    "power_columns = dataset.apply(map_power_values, axis=1)\n",
    "dataset = pd.concat([dataset, power_columns], axis=1)\n",
    "\n",
    "# Save the updated dataset\n",
    "dataset.to_csv(\"rf_dc_mapped_dataset.csv\", index=False)\n",
    "print(\"Mapped dataset saved as rf_dc_mapped_dataset.csv\")\n",
    "\n",
    "# Save the updated dataset\n",
    "dataset.to_csv(\"rf_dc_mapped_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Mapped dataset saved as rf_dc_mapped_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('rf_dc_mapped_dataset.csv')\n",
    "# Define thresholds\n",
    "nan_threshold = 0.2 * len(df)  # Drop columns with more than 40% NaN values\n",
    "negative_threshold = 0.01 * len(df)  # Drop columns where power values < -20 appear too often\n",
    "\n",
    "# Identify power columns\n",
    "power_cols = [col for col in df.columns if col.startswith(\"Power_\")]\n",
    "\n",
    "# Find power columns to drop based on NaN and power value < -20 threshold\n",
    "power_cols_to_drop = [\n",
    "    col for col in power_cols\n",
    "    if df[col].isna().sum() > nan_threshold or (df[col] < -20).sum() > negative_threshold\n",
    "]\n",
    "\n",
    "# Find corresponding frequency columns to drop\n",
    "freq_cols_to_drop = [\n",
    "    col.replace(\"Power_\", \"Freq_\") for col in power_cols_to_drop\n",
    "    if col.replace(\"Power_\", \"Freq_\") in df.columns\n",
    "]\n",
    "\n",
    "# Drop selected power and frequency columns\n",
    "df_cleaned = df.drop(columns=power_cols_to_drop + freq_cols_to_drop)\n",
    "df_dropped_rows = df_cleaned.dropna()\n",
    "df_cleaned=df_dropped_rows\n",
    "df_final=df_dropped_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8846f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the maximum power **for each row separately**\n",
    "# Get the remaining power columns after cleaning\n",
    "remaining_power_cols = [col for col in df_cleaned.columns if col.startswith(\"Power_\")]\n",
    "\n",
    "# Use remaining_power_cols instead of power_cols\n",
    "df_cleaned[\"Max_Power\"] = df_cleaned[remaining_power_cols].max(axis=1)\n",
    "# Compute flatness within 2 dB range\n",
    "def compute_flatness_2db(row):\n",
    "    power_values = row[remaining_power_cols]\n",
    "    max_power = row[\"Max_Power\"]\n",
    "\n",
    "    # Get all power values within 2 dB of max power\n",
    "    power_within_2db = power_values[(max_power - power_values) <= 2]\n",
    "\n",
    "    if power_within_2db.empty or power_within_2db.min() == 0:\n",
    "        return None  # Avoid division by zero\n",
    "\n",
    "    return power_within_2db.max() / power_within_2db.min()  # Compute flatness as ratio\n",
    "\n",
    "df_cleaned[\"Flatness_2dB\"] = df_cleaned.apply(compute_flatness_2db, axis=1)\n",
    "\n",
    "# Count the number of frequency lines where power is within 2 dB of the row's max power\n",
    "# Instead of apply on a subset, use apply on the whole DataFrame\n",
    "df_cleaned[\"Frequency_Lines_2dB\"] = df_cleaned.apply(\n",
    "    lambda row: ((row[\"Max_Power\"] - row[remaining_power_cols]) <= 2).sum(), axis=1\n",
    ")\n",
    "\n",
    "# Find the row with the highest flatness\n",
    "max_flatness_row = df_cleaned.loc[df_cleaned[\"Flatness_2dB\"].idxmax()]\n",
    "\n",
    "# Filter power values where (row-wise Max_Power - Power_n) == 2\n",
    "filtered_data = pd.DataFrame()\n",
    "for col in remaining_power_cols:  # Use remaining_power_cols here as well\n",
    "    freq_col = col.replace(\"Power_\", \"Freq_\")\n",
    "\n",
    "    # Condition check for each row separately\n",
    "    mask = (df_cleaned[\"Max_Power\"] - df_cleaned[col]) == 2\n",
    "    filtered_data[f\"Filtered_{col}\"] = df_cleaned.loc[mask, col]\n",
    "    if freq_col in df_cleaned.columns:\n",
    "        filtered_data[f\"Filtered_{freq_col}\"] = df_cleaned.loc[mask, freq_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abe96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df_dropped_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d206c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new)\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dcd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['RF1','RF2','DC Value']\n",
    "# Get the remaining power columns after cleaning\n",
    "remaining_power_cols = [col for col in df_new.columns if col.startswith(\"Power_\")]\n",
    "# Update targets to only include remaining power columns\n",
    "targets = remaining_power_cols\n",
    "targets=df_final[targets].values\n",
    "features=df_final[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e706c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.1, random_state=42)\n",
    "scaler_features = StandardScaler()\n",
    "X_train = scaler_features.fit_transform(X_train)\n",
    "X_test = scaler_features.transform(X_test)\n",
    "scaler_targets = StandardScaler()\n",
    "y_train = scaler_targets.fit_transform(y_train)\n",
    "y_test = scaler_targets.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f10df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='tanh'),\n",
    "    tf.keras.layers.Dense(64, activation='tanh'),\n",
    "    tf.keras.layers.Dense(32, activation='tanh'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(y_train.shape[1])  # Output layer with the number of targets\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=600, batch_size=8, verbose=1)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = scaler_targets.inverse_transform(y_pred)\n",
    "y_test = scaler_targets.inverse_transform(y_test)\n",
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming predicted_values is a NumPy array of shape (num_samples, num_power_columns)\n",
    "# Get the remaining power columns after cleaning\n",
    "remaining_power_cols = [col for col in df_new.columns if col.startswith(\"Power_\")]\n",
    "# Create DataFrame with columns corresponding to remaining power features\n",
    "# Use the index of df_new to create a range of indices for predicted_powers\n",
    "predicted_powers = pd.DataFrame(y_pred, columns=remaining_power_cols, index=df_new.index[-len(y_pred):]) # Use df_new.index for the index\n",
    "\n",
    "\n",
    "# Create a copy of the original dataset (df_new) that was used for prediction\n",
    "reconstructed_dataset = df_new.copy() #Changed df to df_new\n",
    "# Replace power columns in reconstructed_dataset with predicted values\n",
    "for col_name in remaining_power_cols:\n",
    "    #Use the same index as predicted_powers to align rows\n",
    "    reconstructed_dataset.loc[predicted_powers.index, col_name] = predicted_powers[col_name]  #Assign the predicted values to the corresponding rows in the original dataframe df_new, which was used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstructed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e7741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the maximum power **for each row separately**\n",
    "reconstructed_dataset[\"Max_Power\"] = reconstructed_dataset[remaining_power_cols].max(axis=1)\n",
    "# Compute flatness within 2 dB range\n",
    "def compute_flatness_2db(row):\n",
    "    power_values = row[remaining_power_cols]\n",
    "    max_power = row[\"Max_Power\"]\n",
    "\n",
    "    # Get all power values within 2 dB of max power\n",
    "    power_within_2db = power_values[(max_power - power_values) <= 2]\n",
    "\n",
    "    if power_within_2db.empty or power_within_2db.min() == 0:\n",
    "        return None  # Avoid division by zero\n",
    "\n",
    "    return power_within_2db.max() / power_within_2db.min()  # Compute flatness as ratio\n",
    "\n",
    "reconstructed_dataset[\"Flatness_2dB\"] = reconstructed_dataset.apply(compute_flatness_2db, axis=1)\n",
    "\n",
    "# Count the number of frequency lines where power is within 2 dB of the row's max power\n",
    "reconstructed_dataset[\"Frequency_Lines_2dB\"] = reconstructed_dataset.apply(\n",
    "    lambda row: ((row[\"Max_Power\"] - row[remaining_power_cols]) <= 2).sum(), axis=1\n",
    ")\n",
    "\n",
    "# Find the row with the highest flatness\n",
    "max_flatness_row = reconstructed_dataset.loc[reconstructed_dataset[\"Flatness_2dB\"].idxmax()]\n",
    "\n",
    "# Filter power values where (row-wise Max_Power - Power_n) == 2\n",
    "filtered_data = pd.DataFrame()\n",
    "for col in remaining_power_cols:  # Use remaining_power_cols here as well\n",
    "    freq_col = col.replace(\"Power_\", \"Freq_\")\n",
    "\n",
    "    # Condition check for each row separately\n",
    "    mask = (reconstructed_dataset[\"Max_Power\"] - reconstructed_dataset[col]) == 2\n",
    "    filtered_data[f\"Filtered_{col}\"] = reconstructed_dataset.loc[mask, col]\n",
    "    if freq_col in reconstructed_dataset.columns:\n",
    "        filtered_data[f\"Filtered_{freq_col}\"] = reconstructed_dataset.loc[mask, freq_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_flatness_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstructed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Normalize both columns (min-max normalization)\n",
    "reconstructed_dataset[\"Flatness_2dB_norm\"] = (\n",
    "    reconstructed_dataset[\"Flatness_2dB\"] - reconstructed_dataset[\"Flatness_2dB\"].min()\n",
    ") / (reconstructed_dataset[\"Flatness_2dB\"].max() - reconstructed_dataset[\"Flatness_2dB\"].min())\n",
    "\n",
    "reconstructed_dataset[\"Frequency_Lines_2dB_norm\"] = (\n",
    "    reconstructed_dataset[\"Frequency_Lines_2dB\"] - reconstructed_dataset[\"Frequency_Lines_2dB\"].min()\n",
    ") / (reconstructed_dataset[\"Frequency_Lines_2dB\"].max() - reconstructed_dataset[\"Frequency_Lines_2dB\"].min())\n",
    "\n",
    "# Combine both into a single score (e.g., average of the two normalized scores)\n",
    "reconstructed_dataset[\"Combined_Score\"] = (\n",
    "    reconstructed_dataset[\"Flatness_2dB_norm\"] + reconstructed_dataset[\"Frequency_Lines_2dB_norm\"]\n",
    ") / 2\n",
    "\n",
    "# Find the row with the highest combined score\n",
    "best_row = reconstructed_dataset.loc[reconstructed_dataset[\"Combined_Score\"].idxmax()]\n",
    "\n",
    "# Show the result\n",
    "print(\"Row with both maximum Flatness_2dB and Frequency_Lines_2dB (combined):\")\n",
    "print(best_row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
